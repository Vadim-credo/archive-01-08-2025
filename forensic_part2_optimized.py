# =============================================================================
    # –ú–ï–¢–û–î–ò –†–û–ë–û–¢–ò –ó –î–û–ö–£–ú–ï–ù–¢–ê–ú–ò (–û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–Ü)
    # =============================================================================
    
    @lru_cache(maxsize=500)
    def _get_file_hash(self, file_path, file_size):
        """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–µ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è —Ö–µ—à—É —Ñ–∞–π–ª—É –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                # –ß–∏—Ç–∞—î–º–æ —Ñ–∞–π–ª –±–ª–æ–∫–∞–º–∏ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è —Ö–µ—à—É –¥–ª—è {file_path}: {e}")
            return None
    
    def extract_docx_content(self, file_path, max_size_mb=MAX_FILE_SIZE_MB):
        """
        –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–µ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ DOCX —Ñ–∞–π–ª—ñ–≤
        
        Args:
            file_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É
            max_size_mb: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É –¥–ª—è –æ–±—Ä–æ–±–∫–∏
            
        Returns:
            tuple: (—É—Å–ø—ñ—Ö: bool, —Ç–µ–∫—Å—Ç: str, –ø–æ–º–∏–ª–∫–∞: str)
        """
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É —Ñ–∞–π–ª—É
            file_size_mb = get_file_size_mb(file_path)
            if file_size_mb > max_size_mb:
                return False, "", f"–§–∞–π–ª –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∏–π: {file_size_mb:.1f}MB > {max_size_mb}MB"
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–µ—à—É –∑–∞ —Ö–µ—à–µ–º —Ñ–∞–π–ª—É
            file_hash = self._get_file_hash(file_path, int(file_size_mb * 1024 * 1024))
            if file_hash:
                cache_key = f"docx_content_{file_hash}"
                cached_content = self._load_content_cache(cache_key)
                if cached_content is not None:
                    return True, cached_content, ""
            
            # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É
            doc = Document(file_path)
            
            # –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –∑–±—ñ—Ä —Ç–µ–∫—Å—Ç—É –∑ —É—Å—ñ—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ñ–≤
            paragraphs = []
            for paragraph in doc.paragraphs:
                text = paragraph.text.strip()
                if text:  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏
                    paragraphs.append(text)
            
            # –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—î—é –ø–∞–º'—è—Ç—ñ
            content = '\n'.join(paragraphs)
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –∫–µ—à —è–∫—â–æ —î —Ö–µ—à
            if file_hash and content:
                self._save_content_cache(cache_key, content)
            
            return True, content, ""
            
        except Exception as e:
            error_msg = f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è DOCX {file_path}: {str(e)}"
            print(error_msg)
            return False, "", error_msg
    
    def extract_pdf_content(self, file_path, max_size_mb=MAX_FILE_SIZE_MB):
        """
        –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–µ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ PDF —Ñ–∞–π–ª—ñ–≤
        –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î pdfplumber —è–∫ –æ—Å–Ω–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥, PyPDF2 —è–∫ —Ä–µ–∑–µ—Ä–≤–Ω–∏–π
        
        Args:
            file_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É
            max_size_mb: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É –¥–ª—è –æ–±—Ä–æ–±–∫–∏
            
        Returns:
            tuple: (—É—Å–ø—ñ—Ö: bool, —Ç–µ–∫—Å—Ç: str, –ø–æ–º–∏–ª–∫–∞: str)
        """
        if not PDF_AVAILABLE:
            return False, "", "PDF –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ñ"
        
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É —Ñ–∞–π–ª—É
            file_size_mb = get_file_size_mb(file_path)
            if file_size_mb > max_size_mb:
                return False, "", f"PDF —Ñ–∞–π–ª –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∏–π: {file_size_mb:.1f}MB > {max_size_mb}MB"
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–µ—à—É
            file_hash = self._get_file_hash(file_path, int(file_size_mb * 1024 * 1024))
            if file_hash:
                cache_key = f"pdf_content_{file_hash}"
                cached_content = self._load_content_cache(cache_key)
                if cached_content is not None:
                    return True, cached_content, ""
            
            # –°–ø—Ä–æ–±—É—î–º–æ pdfplumber (–∫—Ä–∞—â–µ —è–∫—ñ—Å—Ç—å —Ç–µ–∫—Å—Ç—É)
            success, content, error = self._extract_pdf_pdfplumber(file_path)
            
            # –Ø–∫—â–æ pdfplumber –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–≤, —Å–ø—Ä–æ–±—É—î–º–æ PyPDF2
            if not success:
                success, content, error = self._extract_pdf_pypdf2(file_path)
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –∫–µ—à –ø—Ä–∏ —É—Å–ø—ñ—Ö—É
            if success and file_hash and content:
                self._save_content_cache(cache_key, content)
            
            return success, content, error
            
        except Exception as e:
            error_msg = f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è PDF {file_path}: {str(e)}"
            print(error_msg)
            return False, "", error_msg
    
    def _extract_pdf_pdfplumber(self, file_path):
        """–í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é pdfplumber"""
        try:
            import pdfplumber
            
            pages_text = []
            with pdfplumber.open(file_path) as pdf:
                # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö —Ñ–∞–π–ª—ñ–≤
                max_pages = min(len(pdf.pages), 100)  
                
                for i, page in enumerate(pdf.pages[:max_pages]):
                    try:
                        text = page.extract_text()
                        if text and text.strip():
                            pages_text.append(text.strip())
                    except Exception as page_error:
                        print(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {i+1}: {page_error}")
                        continue
            
            content = '\n\n'.join(pages_text)
            return True, content, ""
            
        except Exception as e:
            return False, "", f"pdfplumber –ø–æ–º–∏–ª–∫–∞: {str(e)}"
    
    def _extract_pdf_pypdf2(self, file_path):
        """–†–µ–∑–µ—Ä–≤–Ω–µ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é PyPDF2"""
        try:
            import PyPDF2
            
            pages_text = []
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–æ—Ä—ñ–Ω–æ–∫
                max_pages = min(len(pdf_reader.pages), 100)
                
                for i in range(max_pages):
                    try:
                        page = pdf_reader.pages[i]
                        text = page.extract_text()
                        if text and text.strip():
                            pages_text.append(text.strip())
                    except Exception as page_error:
                        print(f"PyPDF2: –ø–æ–º–∏–ª–∫–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {i+1}: {page_error}")
                        continue
            
            content = '\n\n'.join(pages_text)
            return True, content, ""
            
        except Exception as e:
            return False, "", f"PyPDF2 –ø–æ–º–∏–ª–∫–∞: {str(e)}"
    
    def _save_content_cache(self, cache_key, content):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É –≤ –∫–µ—à –∑ –∫–æ–º–ø—Ä–µ—Å—ñ—î—é"""
        if not self._cache_initialized:
            return False
            
        cache_file = os.path.join(CACHE_DIR, f"content_{cache_key}.pkl")
        try:
            # –ö–æ–º–ø—Ä–µ—Å—É—î–º–æ –≤–µ–ª–∏–∫—ñ —Ç–µ–∫—Å—Ç–∏
            if len(content) > 10000:
                import gzip
                with gzip.open(cache_file + '.gz', 'wb') as f:
                    pickle.dump(content, f, protocol=pickle.HIGHEST_PROTOCOL)
            else:
                with open(cache_file, 'wb') as f:
                    pickle.dump(content, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            self.cache_timestamps[cache_key] = time.time()
            return True
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É –≤ –∫–µ—à: {e}")
            return False
    
    def _load_content_cache(self, cache_key):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É –∑ –∫–µ—à—É –∑ —Ä–æ–∑–ø–∞–∫–æ–≤–∫–æ—é"""
        if not self._cache_initialized:
            return None
            
        cache_file = os.path.join(CACHE_DIR, f"content_{cache_key}.pkl")
        cache_file_gz = cache_file + '.gz'
        
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—ñ –∫–µ—à—É
            if not self.is_cache_valid(cache_key):
                return None
            
            # –°–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–∏—Å–Ω–µ–Ω–∏–π —Ñ–∞–π–ª
            if os.path.exists(cache_file_gz):
                import gzip
                with gzip.open(cache_file_gz, 'rb') as f:
                    return pickle.load(f)
            
            # –ê–±–æ –∑–≤–∏—á–∞–π–Ω–∏–π —Ñ–∞–π–ª
            if os.path.exists(cache_file):
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
                    
            return None
            
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É –∑ –∫–µ—à—É: {e}")
            # –í–∏–¥–∞–ª—è—î–º–æ –ø–æ—à–∫–æ–¥–∂–µ–Ω—ñ —Ñ–∞–π–ª–∏
            for f in [cache_file, cache_file_gz]:
                try:
                    if os.path.exists(f):
                        os.remove(f)
                except:
                    pass
            return None

    # =============================================================================
    # –ü–ê–†–°–ò–ù–ì –ï–ö–°–ü–ï–†–¢–ò–ó (–û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–ò–ô)
    # =============================================================================
    
    def parse_expertise_document(self, file_path, content=None):
        """
        –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏ –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–º —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è–º
        
        Args:
            file_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É
            content: –ì–æ—Ç–æ–≤–∏–π —Ç–µ–∫—Å—Ç (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ —á–∏—Ç–∞–Ω–Ω—è)
            
        Returns:
            dict: –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–æ–∑–ø—ñ–∑–Ω–∞–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏
        """
        try:
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É —è–∫—â–æ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–æ
            if content is None:
                content = self._extract_document_content(file_path)
                if not content:
                    return self._get_default_expertise_data(file_path)
            
            # –ë–∞–∑–æ–≤–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
            result = self._get_default_expertise_data(file_path)
            
            # –ü–æ–∫—Ä–∞—â–µ–Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –∑ –º–Ω–æ–∂–∏–Ω–Ω–∏–º–∏ –ø–∞—Ç–µ—Ä–Ω–∞–º–∏
            result.update(self._parse_erddr_patterns(content))
            result.update(self._parse_expertise_number_patterns(content))
            result.update(self._parse_date_patterns(content))
            result.update(self._parse_expert_patterns(content))
            result.update(self._determine_expertise_type(content))
            result.update(self._determine_sector(content, result.get('expertise_type', '')))
            
            # –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ç–∞ –æ—á–∏—â–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
            result = self._validate_and_clean_expertise_data(result)
            
            return result
            
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É {file_path}: {e}")
            return self._get_default_expertise_data(file_path)
    
    def _extract_document_content(self, file_path):
        """–í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è–º —Ç–∏–ø—É"""
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext == '.docx':
            success, content, _ = self.extract_docx_content(file_path)
            return content if success else ""
        elif file_ext == '.pdf':
            success, content, _ = self.extract_pdf_content(file_path)
            return content if success else ""
        else:
            return ""
    
    def _get_default_expertise_data(self, file_path):
        """–ë–∞–∑–æ–≤–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–∏—Ö –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏"""
        return {
            'erddr_number': None,
            'expertise_number': None,
            'expertise_date': None,
            'expertise_year': None,
            'expertise_type': '–Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ',
            'expert_name': '–ù–µ–≤—ñ–¥–æ–º–∏–π_–µ–∫—Å–ø–µ—Ä—Ç',
            'sector': '–ø–æ—á–µ—Ä–∫ —Ç–∞ –¢–ï–î',
            'source_file': os.path.basename(file_path),
            'file_path': file_path,
            'file_size': int(get_file_size_mb(file_path) * 1024 * 1024)
        }
    
    def _parse_erddr_patterns(self, content):
        """–ü–æ–∫—Ä–∞—â–µ–Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –Ω–æ–º–µ—Ä—ñ–≤ –Ñ–†–î–† –∑ –º–Ω–æ–∂–∏–Ω–Ω–∏–º–∏ –ø–∞—Ç–µ—Ä–Ω–∞–º–∏"""
        patterns = [
            r'–Ñ–†–î–†\s*[‚Ññ#]\s*(\d{11,12})',
            r'–Ñ–†–î–†\s*(\d{11,12})',
            r'‚Ññ\s*(\d{11,12})',
            r'—Å–ø—Ä–∞–≤[–∞—ñ]\s*‚Ññ\s*(\d{11,12})',
            r'–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω[—è—ñ]\s*‚Ññ\s*(\d{11,12})',
            r'(\d{11,12})\s*–≤—ñ–¥',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                erddr = match.group(1)
                # –í–∞–ª—ñ–¥–∞—Ü—ñ—è –¥–æ–≤–∂–∏–Ω–∏ –Ñ–†–î–†
                if 11 <= len(erddr) <= 12:
                    return {'erddr_number': erddr}
        
        return {'erddr_number': None}
    
    def _parse_expertise_number_patterns(self, content):
        """–ü–æ–∫—Ä–∞—â–µ–Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –Ω–æ–º–µ—Ä—ñ–≤ –µ–∫—Å–ø–µ—Ä—Ç–∏–∑"""
        patterns = [
            r'–ï–∫—Å–ø–µ—Ä—Ç–∏–∑[–∞—ñ]\s*‚Ññ\s*(\d+(?:/\d+)*)',
            r'–í–∏—Å–Ω–æ–≤–æ–∫\s*–µ–∫—Å–ø–µ—Ä—Ç[–∞–∏]\s*‚Ññ\s*(\d+(?:/\d+)*)',
            r'‚Ññ\s*(\d+(?:/\d+)*)\s*–≤—ñ–¥\s*\d+\.\d+\.\d+',
            r'–µ–∫—Å–ø–µ—Ä—Ç–∏–∑[–∞—ñ]\s*(\d+(?:/\d+)*)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                number = match.group(1)
                return {'expertise_number': number}
        
        return {'expertise_number': None}
    
    def _parse_date_patterns(self, content):
        """–ü–æ–∫—Ä–∞—â–µ–Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –¥–∞—Ç –∑ –º–Ω–æ–∂–∏–Ω–Ω–∏–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏"""
        date_patterns = [
            r'–≤—ñ–¥\s*(\d{1,2})\.(\d{1,2})\.(\d{4})',
            r'(\d{1,2})\.(\d{1,2})\.(\d{4})\s*—Ä\.?',
            r'(\d{4})\s*—Ä–æ–∫[–∏—É]',
            r'(\d{1,2})\s+(—Å—ñ—á–Ω—è|–ª—é—Ç–æ–≥–æ|–±–µ—Ä–µ–∑–Ω—è|–∫–≤—ñ—Ç–Ω—è|—Ç—Ä–∞–≤–Ω—è|—á–µ—Ä–≤–Ω—è|–ª–∏–ø–Ω—è|—Å–µ—Ä–ø–Ω—è|–≤–µ—Ä–µ—Å–Ω—è|–∂–æ–≤—Ç–Ω—è|–ª–∏—Å—Ç–æ–ø–∞–¥–∞|–≥—Ä—É–¥–Ω—è)\s+(\d{4})',
        ]
        
        months_ua = {
            '—Å—ñ—á–Ω—è': '01', '–ª—é—Ç–æ–≥–æ': '02', '–±–µ—Ä–µ–∑–Ω—è': '03', '–∫–≤—ñ—Ç–Ω—è': '04',
            '—Ç—Ä–∞–≤–Ω—è': '05', '—á–µ—Ä–≤–Ω—è': '06', '–ª–∏–ø–Ω—è': '07', '—Å–µ—Ä–ø–Ω—è': '08',
            '–≤–µ—Ä–µ—Å–Ω—è': '09', '–∂–æ–≤—Ç–Ω—è': '10', '–ª–∏—Å—Ç–æ–ø–∞–¥–∞': '11', '–≥—Ä—É–¥–Ω—è': '12'
        }
        
        for pattern in date_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                groups = match.groups()
                
                if len(groups) == 3:
                    if groups[1] in months_ua:  # –£–∫—Ä–∞—ó–Ω—Å—å–∫—ñ –Ω–∞–∑–≤–∏ –º—ñ—Å—è—Ü—ñ–≤
                        day, month_name, year = groups
                        month = months_ua[groups[1]]
                        try:
                            date_str = f"{day.zfill(2)}.{month}.{year}"
                            return {
                                'expertise_date': date_str,
                                'expertise_year': int(year)
                            }
                        except:
                            continue
                    else:  # –¶–∏—Ñ—Ä–æ–≤–∏–π —Ñ–æ—Ä–º–∞—Ç
                        try:
                            day, month, year = groups
                            date_str = f"{day.zfill(2)}.{month.zfill(2)}.{year}"
                            return {
                                'expertise_date': date_str,
                                'expertise_year': int(year)
                            }
                        except:
                            continue
        
        return {'expertise_date': None, 'expertise_year': None}
    
    def _parse_expert_patterns(self, content):
        """–ü–æ–∫—Ä–∞—â–µ–Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è —ñ–º–µ–Ω –µ–∫—Å–ø–µ—Ä—Ç—ñ–≤"""
        patterns = [
            r'–ï–∫—Å–ø–µ—Ä—Ç[:\s]+([–ê-–Ø–Ü–á–Ñ“ê][–∞-—è—ñ—ó—î“ë]+\s+[–ê-–Ø–Ü–á–Ñ“ê]\.[–ê-–Ø–Ü–á–Ñ“ê]\.)',
            r'([–ê-–Ø–Ü–á–Ñ“ê][–∞-—è—ñ—ó—î“ë]+\s+[–ê-–Ø–Ü–á–Ñ“ê]\.[–ê-–Ø–Ü–á–Ñ“ê]\.)\s*(?:–µ–∫—Å–ø–µ—Ä—Ç|–ø—ñ–¥–ø–∏—Å)',
            r'–í–∏–∫–æ–Ω–∞–≤[:\s]+([–ê-–Ø–Ü–á–Ñ“ê][–∞-—è—ñ—ó—î“ë]+\s+[–ê-–Ø–Ü–á–Ñ“ê]\.[–ê-–Ø–Ü–á–Ñ“ê]\.)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                expert_name = match.group(1).strip()
                # –û—á–∏—â–µ–Ω–Ω—è —Ç–∞ —Ñ–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —ñ–º–µ–Ω—ñ
                expert_name = re.sub(r'\s+', '_', expert_name)
                return {'expert_name': expert_name}
        
        return {'expert_name': '–ù–µ–≤—ñ–¥–æ–º–∏–π_–µ–∫—Å–ø–µ—Ä—Ç'}
    
    def _determine_expertise_type(self, content):
        """–ü–æ–∫—Ä–∞—â–µ–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏ –∑ –≤–∞–≥–∞–º–∏"""
        content_lower = content.lower()
        
        # –°–ª–æ–≤–Ω–∏–∫ –∑ —Ç–∏–ø–∞–º–∏ –µ–∫—Å–ø–µ—Ä—Ç–∏–∑ —Ç–∞ —ó—Ö –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –∑ –≤–∞–≥–∞–º–∏
        expertise_weights = {}
        
        for expertise_type, keywords in self.expertise_keywords.items():
            weight = 0
            for keyword in keywords:
                # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –≤—Ö–æ–¥–∂–µ–Ω—å –∑ –≤—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
                count = content_lower.count(keyword.lower())
                # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –≤–∞–≥–∏ –¥–ª—è –∫–ª—é—á–æ–≤–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ–≤
                if re.search(rf'{keyword.lower()}\s*(–µ–∫—Å–ø–µ—Ä—Ç–∏–∑|–¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω|–≤–∏—Å–Ω–æ–≤–æ–∫)', content_lower):
                    weight += count * 3
                else:
                    weight += count
            
            if weight > 0:
                expertise_weights[expertise_type] = weight
        
        if expertise_weights:
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç–∏–ø –∑ –Ω–∞–π–±—ñ–ª—å—à–æ—é –≤–∞–≥–æ—é
            best_type = max(expertise_weights, key=expertise_weights.get)
            return {'expertise_type': best_type}
        
        return {'expertise_type': '–Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ'}
    
    def _determine_sector(self, content, expertise_type):
        """–í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å–µ–∫—Ç–æ—Ä—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç–∏–ø—É –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏"""
        # –°–ø–æ—á–∞—Ç–∫—É –Ω–∞–º–∞–≥–∞—î–º–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –∑–∞ —Ç–∏–ø–æ–º –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏
        type_to_sector = {
            '–ø–æ—á–µ—Ä–∫–æ–∑–Ω–∞–≤—á–∞': '–ø–æ—á–µ—Ä–∫ —Ç–∞ –¢–ï–î',
            '–∑–±—Ä–æ—ó': '–±–∞–ª—ñ—Å—Ç–∏',
            '—Ç—Ä–∞—Å–æ–ª–æ–≥—ñ—á–Ω–∞': '—Ç—Ä–∞—Å–æ–ª–æ–≥—ñ—è',
            '–¥–∞–∫—Ç–∏–ª–æ—Å–∫–æ–ø—ñ—á–Ω–∞': '–¥–∞–∫—Ç–∏–ª–æ—Å–∫–æ–ø—ñ—è',
            '–±–∞–ª. –æ–±–ª—ñ–∫': '–±–∞–ª. –æ–±–ª—ñ–∫'
        }
        
        sector = type_to_sector.get(expertise_type, '–ø–æ—á–µ—Ä–∫ —Ç–∞ –¢–ï–î')
        
        return {'sector': sector}
    
    def _validate_and_clean_expertise_data(self, data):
        """–í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ç–∞ –æ—á–∏—â–µ–Ω–Ω—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
        # –û—á–∏—â–µ–Ω–Ω—è –Ñ–†–î–†
        if data.get('erddr_number'):
            erddr = re.sub(r'[^\d]', '', str(data['erddr_number']))
            if not (11 <= len(erddr) <= 12):
                data['erddr_number'] = None
            else:
                data['erddr_number'] = erddr
        
        # –û—á–∏—â–µ–Ω–Ω—è –Ω–æ–º–µ—Ä–∞ –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏
        if data.get('expertise_number'):
            number = str(data['expertise_number']).strip()
            if not re.match(r'^\d+(/\d+)*$', number):
                data['expertise_number'] = None
        
        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ä–æ–∫—É
        if data.get('expertise_year'):
            try:
                year = int(data['expertise_year'])
                current_year = datetime.now().year
                if not (2000 <= year <= current_year + 1):
                    data['expertise_year'] = None
                    data['expertise_date'] = None
            except (ValueError, TypeError):
                data['expertise_year'] = None
                data['expertise_date'] = None
        
        # –û—á–∏—â–µ–Ω–Ω—è —ñ–º–µ–Ω—ñ –µ–∫—Å–ø–µ—Ä—Ç–∞
        if data.get('expert_name'):
            name = str(data['expert_name']).strip()
            if len(name) < 3:
                data['expert_name'] = '–ù–µ–≤—ñ–¥–æ–º–∏–π_–µ–∫—Å–ø–µ—Ä—Ç'
        
        return data

    # =============================================================================
    # –ü–û–®–£–ö –î–û–ö–£–ú–ï–ù–¢–Ü–í (–û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–ò–ô)
    # =============================================================================
    
    def _search_documents_impl(self, erddr_number=None, expertise_number=None, 
                              expertise_date=None, expertise_year=None, 
                              expert_name=None, sector=None, expertise_type=None,
                              limit=1000, use_cache=True):
        """
        –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –ø–æ—à—É–∫ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É —Ç–∞ —ñ–Ω–¥–µ–∫—Å—ñ–≤
        
        Args:
            erddr_number: –ù–æ–º–µ—Ä –Ñ–†–î–†
            expertise_number: –ù–æ–º–µ—Ä –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏
            expertise_date: –î–∞—Ç–∞ –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏
            expertise_year: –†—ñ–∫ –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏
            expert_name: –Ü–º'—è –µ–∫—Å–ø–µ—Ä—Ç–∞
            sector: –°–µ–∫—Ç–æ—Ä
            expertise_type: –¢–∏–ø –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏
            limit: –û–±–º–µ–∂–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            use_cache: –ß–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∫–µ—à
            
        Returns:
            pd.DataFrame: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–æ—à—É–∫—É
        """
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–ª—é—á–∞ –∫–µ—à—É
            cache_key = None
            if use_cache:
                cache_key = self.get_cache_key(
                    erddr=erddr_number, number=expertise_number, 
                    date=expertise_date, year=expertise_year,
                    expert=expert_name, sector=sector, type=expertise_type,
                    limit=limit
                )
                
                # –°–ø—Ä–æ–±–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑ –∫–µ—à—É
                cached_results = self.load_search_cache(cache_key)
                if cached_results is not None:
                    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ –∑ –∫–µ—à—É ({len(cached_results)} –∑–∞–ø–∏—Å—ñ–≤)")
                    return cached_results
            
            # –ü–æ–±—É–¥–æ–≤–∞ SQL –∑–∞–ø–∏—Ç—É –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—î—é
            query, params = self._build_optimized_search_query(
                erddr_number, expertise_number, expertise_date, expertise_year,
                expert_name, sector, expertise_type, limit
            )
            
            # –í–∏–∫–æ–Ω–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É
            conn = sqlite3.connect(self.db_path)
            
            # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –¥–ª—è —á–∏—Ç–∞–Ω–Ω—è
            conn.execute("PRAGMA temp_store = memory")
            conn.execute("PRAGMA mmap_size = 268435456")  # 256MB
            
            results_df = pd.read_sql_query(query, conn, params=params)
            conn.close()
            
            print(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(results_df)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –∫–µ—à
            if use_cache and cache_key and not results_df.empty:
                self.save_search_cache(cache_key, results_df)
            
            return results_df
            
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤: {str(e)}")
            return pd.DataFrame()
    
    def _build_optimized_search_query(self, erddr_number, expertise_number, 
                                    expertise_date, expertise_year, expert_name, 
                                    sector, expertise_type, limit):
        """
        –ü–æ–±—É–¥–æ–≤–∞ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ–≥–æ SQL –∑–∞–ø–∏—Ç—É –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º —ñ–Ω–¥–µ–∫—Å—ñ–≤
        """
        # –ë–∞–∑–æ–≤–∏–π –∑–∞–ø–∏—Ç
        base_query = """
            SELECT id, erddr_number, expertise_number, expertise_date, 
                   expertise_year, expertise_type, expert_name, sector,
                   source_file, file_path, file_size, created_at
            FROM expertise_cases
        """
        
        conditions = []
        params = {}
        
        # –£–º–æ–≤–∏ –ø–æ—à—É–∫—É –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—î—é —ñ–Ω–¥–µ–∫—Å—ñ–≤
        if erddr_number:
            conditions.append("erddr_number = :erddr")
            params['erddr'] = erddr_number
        
        if expertise_number:
            conditions.append("expertise_number = :number")
            params['number'] = expertise_number
        
        if expertise_date:
            conditions.append("expertise_date = :date")
            params['date'] = expertise_date
        
        if expertise_year:
            conditions.append("expertise_year = :year")
            params['year'] = expertise_year
        
        if expert_name:
            conditions.append("expert_name LIKE :expert")
            params['expert'] = f"%{expert_name}%"
        
        if sector:
            conditions.append("sector = :sector")
            params['sector'] = sector
        
        if expertise_type and expertise_type != '–Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ':
            conditions.append("expertise_type = :type")
            params['type'] = expertise_type
        
        # –ó–±—ñ—Ä–∫–∞ –∑–∞–ø–∏—Ç—É
        if conditions:
            query = base_query + " WHERE " + " AND ".join(conditions)
        else:
            query = base_query
        
        # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –æ–±–º–µ–∂–µ–Ω–Ω—è
        query += " ORDER BY expertise_year DESC, expertise_date DESC, id DESC"
        
        if limit:
            query += f" LIMIT {limit}"
        
        return query, params
    
    def get_search_statistics(self):
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –ø–æ—à—É–∫—É"""
        try:
            self._ensure_database_initialized()
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            stats = {}
            
            # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤
            cursor.execute("SELECT COUNT(*) FROM expertise_cases")
            stats['total_records'] = cursor.fetchone()[0]
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–æ–∫–∞—Ö
            cursor.execute("""
                SELECT expertise_year, COUNT(*) 
                FROM expertise_cases 
                WHERE expertise_year IS NOT NULL 
                GROUP BY expertise_year 
                ORDER BY expertise_year DESC
            """)
            stats['by_year'] = dict(cursor.fetchall())
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –µ–∫—Å–ø–µ—Ä—Ç–∞–º
            cursor.execute("""
                SELECT expert_name, COUNT(*) 
                FROM expertise_cases 
                GROUP BY expert_name 
                ORDER BY COUNT(*) DESC 
                LIMIT 10
            """)
            stats['by_expert'] = dict(cursor.fetchall())
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–µ–∫—Ç–æ—Ä–∞–º
            cursor.execute("""
                SELECT sector, COUNT(*) 
                FROM expertise_cases 
                GROUP BY sector 
                ORDER BY COUNT(*) DESC
            """)
            stats['by_sector'] = dict(cursor.fetchall())
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –µ–∫—Å–ø–µ—Ä—Ç–∏–∑
            cursor.execute("""
                SELECT expertise_type, COUNT(*) 
                FROM expertise_cases 
                GROUP BY expertise_type 
                ORDER BY COUNT(*) DESC
            """)
            stats['by_type'] = dict(cursor.fetchall())
            
            conn.close()
            return stats
            
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}

    # =============================================================================
    # –î–û–î–ê–í–ê–ù–ù–Ø –î–û–ö–£–ú–ï–ù–¢–Ü–í (–û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–Ü –ú–ï–¢–û–î–ò)
    # =============================================================================
    
    def _add_document_impl(self, file_path, force_reparse=False, **override_data):
        """
        –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–µ –¥–æ–¥–∞–≤–∞–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∞—Ä—Ö—ñ–≤ –∑ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
        
        Args:
            file_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É
            force_reparse: –ü—Ä–∏–º—É—Å–æ–≤–∏–π –ø–æ–≤—Ç–æ—Ä–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥
            **override_data: –î–∞–Ω—ñ –¥–ª—è –ø–µ—Ä–µ–≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —Ä–æ–∑–ø—ñ–∑–Ω–∞–Ω–∏—Ö
            
        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–ø–µ—Ä–∞—Ü—ñ—ó
        """
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É
            if not os.path.exists(file_path):
                return {'success': False, 'error': f'–§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {file_path}'}
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç—É —Ñ–∞–π–ª—É
            if not file_path.lower().endswith(SUPPORTED_EXTENSIONS):
                return {'success': False, 'error': f'–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª—É'}
            
            # –û–±—á–∏—Å–ª–µ–Ω–Ω—è —Ö–µ—à—É –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
            file_size = int(get_file_size_mb(file_path) * 1024 * 1024)
            file_hash = self._get_file_hash(file_path, file_size)
            
            if not force_reparse and file_hash:
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —Ñ–∞–π–ª –≤–∂–µ —ñ—Å–Ω—É—î –≤ –±–∞–∑—ñ
                existing_doc = self._check_duplicate_document(file_hash, file_path)
                if existing_doc:
                    return {
                        'success': False, 
                        'error': f'–§–∞–π–ª –≤–∂–µ —ñ—Å–Ω—É—î –≤ –±–∞–∑—ñ (ID: {existing_doc["id"]})',
                        'existing_id': existing_doc['id']
                    }
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            parsed_data = self.parse_expertise_document(file_path)
            
            # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–µ—Ä–µ–≤–∏–∑–Ω–∞—á–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö
            parsed_data.update(override_data)
            
            # –î–æ–¥–∞–≤–∞–Ω–Ω—è —Ö–µ—à—É
            parsed_data['file_hash'] = file_hash
            
            # –ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É –≤ –∞—Ä—Ö—ñ–≤ (—è–∫—â–æ –Ω–µ –≤ —Ä–µ–∂–∏–º—ñ —ñ–Ω–¥–µ–∫—Å—É–≤–∞–Ω–Ω—è)
            archive_path = file_path  # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –∑–∞–ª–∏—à–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —à–ª—è—Ö
            
            if not self.index_only_mode:
                copy_result = self._copy_file_to_archive(file_path, parsed_data)
                if copy_result['success']:
                    archive_path = copy_result['archive_path']
                else:
                    return copy_result  # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø–æ–º–∏–ª–∫—É –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è
            
            # –û–Ω–æ–≤–ª–µ–Ω–Ω—è —à–ª—è—Ö—É —Ñ–∞–π–ª—É
            parsed_data['file_path'] = archive_path
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö
            doc_id = self._save_document_to_database(parsed_data)
            
            if doc_id:
                # –û—á–∏—â–µ–Ω–Ω—è –∫–µ—à—É –ø–æ—à—É–∫—É –ø—ñ—Å–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è
                self._invalidate_search_cache()
                
                return {
                    'success': True,
                    'document_id': doc_id,
                    'archive_path': archive_path,
                    'parsed_data': parsed_data
                }
            else:
                return {'success': False, 'error': '–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö'}
                
        except Exception as e:
            error_msg = f"–ü–æ–º–∏–ª–∫–∞ –¥–æ–¥–∞–≤–∞–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {file_path}: {str(e)}"
            print(error_msg)
            return {'success': False, 'error': error_msg}
    
    def _check_duplicate_document(self, file_hash, file_path):
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∑–∞ —Ö–µ—à–µ–º –∞–±–æ —à–ª—è—Ö–æ–º"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –ü–æ—à—É–∫ –∑–∞ —Ö–µ—à–µ–º (–Ω–∞–π–Ω–∞–¥—ñ–π–Ω—ñ—à–∏–π –º–µ—Ç–æ–¥)
            if file_hash:
                cursor.execute(
                    "SELECT id, file_path, source_file FROM expertise_cases WHERE file_hash = ?",
                    (file_hash,)
                )
                result = cursor.fetchone()
                if result:
                    conn.close()
                    return {'id': result[0], 'file_path': result[1], 'source_file': result[2]}
            
            # –î–æ–¥–∞—Ç–∫–æ–≤–∏–π –ø–æ—à—É–∫ –∑–∞ –Ω–∞–∑–≤–æ—é —Ñ–∞–π–ª—É (–º–µ–Ω—à –Ω–∞–¥—ñ–π–Ω–∏–π)
            filename = os.path.basename(file_path)
            cursor.execute(
                "SELECT id, file_path, source_file FROM expertise_cases WHERE source_file = ?",
                (filename,)
            )
            result = cursor.fetchone()
            conn.close()
            
            if result:
                return {'id': result[0], 'file_path': result[1], 'source_file': result[2]}
            
            return None
            
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {e}")
            return None
    
    def _copy_file_to_archive(self, source_path, parsed_data):
        """
        –ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞—Ä—Ö—ñ–≤—É –∑ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—î—é —É–Ω—ñ–∫–∞–ª—å–Ω–æ–≥–æ —ñ–º–µ–Ω—ñ
        """
        try:
            # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å–µ–∫—Ç–æ—Ä—É —Ç–∞ –µ–∫—Å–ø–µ—Ä—Ç–∞
            sector = parsed_data.get('sector', '–ø–æ—á–µ—Ä–∫ —Ç–∞ –¢–ï–î')
            expert_name = parsed_data.get('expert_name', '–ù–µ–≤—ñ–¥–æ–º–∏–π_–µ–∫—Å–ø–µ—Ä—Ç')
            expertise_year = parsed_data.get('expertise_year')
            
            # –ü–æ–±—É–¥–æ–≤–∞ —à–ª—è—Ö—É –¥–æ –∞—Ä—Ö—ñ–≤—É
            sector_path = os.path.join(self.archive_folder, sector)
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞–ø–∫–∏ –µ–∫—Å–ø–µ—Ä—Ç–∞
            expert_path = os.path.join(sector_path, expert_name)
            if not ensure_directory_exists(expert_path):
                return {'success': False, 'error': f'–ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø–∞–ø–∫—É –µ–∫—Å–ø–µ—Ä—Ç–∞: {expert_path}'}
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞–ø–∫–∏ —Ä–æ–∫—É (—è–∫—â–æ —Ä—ñ–∫ –≤–∏–∑–Ω–∞—á–µ–Ω–æ)
            if expertise_year:
                year_path = os.path.join(expert_path, str(expertise_year))
                if not ensure_directory_exists(year_path):
                    print(f"–ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è: –Ω–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø–∞–ø–∫—É —Ä–æ–∫—É {expertise_year}")
                    target_dir = expert_path
                else:
                    target_dir = year_path
            else:
                target_dir = expert_path
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ–≥–æ —ñ–º–µ–Ω—ñ —Ñ–∞–π–ª—É
            original_filename = os.path.basename(source_path)
            name, ext = os.path.splitext(original_filename)
            
            # –î–æ–¥–∞–≤–∞–Ω–Ω—è –ø—Ä–µ—Ñ—ñ–∫—Å—É –∑ –Ω–æ–º–µ—Ä–æ–º –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–∏ —è–∫—â–æ —î
            expertise_number = parsed_data.get('expertise_number')
            if expertise_number:
                name = f"{expertise_number}_{name}"
            
            target_filename = f"{name}{ext}"
            target_path = os.path.join(target_dir, target_filename)
            
            # –û–±—Ä–æ–±–∫–∞ –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—ñ–≤ —ñ–º–µ–Ω —Ñ–∞–π–ª—ñ–≤
            counter = 1
            while os.path.exists(target_path):
                target_filename = f"{name}_{counter}{ext}"
                target_path = os.path.join(target_dir, target_filename)
                counter += 1
            
            # –ö–æ–ø—ñ—é–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É
            shutil.copy2(source_path, target_path)
            
            return {
                'success': True,
                'archive_path': target_path,
                'relative_path': os.path.relpath(target_path, self.archive_folder)
            }
            
        except Exception as e:
            error_msg = f"–ü–æ–º–∏–ª–∫–∞ –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É: {str(e)}"
            print(error_msg)
            return {'success': False, 'error': error_msg}
    
    def _save_document_to_database(self, data):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—î—é"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            insert_data = {
                'erddr_number': data.get('erddr_number'),
                'expertise_number': data.get('expertise_number'),
                'expertise_date': data.get('expertise_date'),
                'expertise_year': data.get('expertise_year'),
                'expertise_type': data.get('expertise_type'),
                'expert_name': data.get('expert_name'),
                'sector': data.get('sector'),
                'source_file': data.get('source_file'),
                'file_path': data.get('file_path'),
                'file_size': data.get('file_size', 0),
                'file_hash': data.get('file_hash'),
            }
            
            # SQL –∑–∞–ø–∏—Ç –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            cursor.execute('''
                INSERT INTO expertise_cases (
                    erddr_number, expertise_number, expertise_date, expertise_year,
                    expertise_type, expert_name, sector, source_file, file_path,
                    file_size, file_hash, created_at, updated_at
                ) VALUES (
                    :erddr_number, :expertise_number, :expertise_date, :expertise_year,
                    :expertise_type, :expert_name, :sector, :source_file, :file_path,
                    :file_size, :file_hash, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP
                )
            ''', insert_data)
            
            doc_id = cursor.lastrowid
            conn.commit()
            conn.close()
            
            print(f"–î–æ–∫—É–º–µ–Ω—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö –∑ ID: {doc_id}")
            return doc_id
            
        except sqlite3.IntegrityError as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ —É–Ω—ñ–∫–∞–ª—å–Ω–æ—Å—Ç—ñ: {e}")
            return None
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö: {e}")
            return None
    
    def _invalidate_search_cache(self):
        """–û—á–∏—â–µ–Ω–Ω—è –∫–µ—à—É –ø–æ—à—É–∫—É –ø—ñ—Å–ª—è –∑–º—ñ–Ω —É –±–∞–∑—ñ –¥–∞–Ω–∏—Ö"""
        try:
            if not self._cache_initialized:
                return
            
            # –û—á–∏—â–µ–Ω–Ω—è –∫–µ—à—É —É –ø–∞–º'—è—Ç—ñ
            self.search_cache.clear()
            
            # –û—á–∏—â–µ–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤ –∫–µ—à—É –ø–æ—à—É–∫—É
            if os.path.exists(CACHE_DIR):
                for filename in os.listdir(CACHE_DIR):
                    if filename.startswith('search_') and filename.endswith('.pkl'):
                        try:
                            os.remove(os.path.join(CACHE_DIR, filename))
                        except:
                            pass
            
            print("–ö–µ—à –ø–æ—à—É–∫—É –æ—á–∏—â–µ–Ω–æ")
            
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è –∫–µ—à—É: {e}")

    # =============================================================================
    # –°–ö–ê–ù–£–í–ê–ù–ù–Ø –Ü–°–ù–£–Æ–ß–û–ì–û –ê–†–•–Ü–í–£ (–û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–Ü –ú–ï–¢–û–î–ò)
    # =============================================================================
    
    def _scan_existing_archive_impl(self, archive_path, progress_callback=None, 
                                   batch_size=50, skip_existing=True):
        """
        –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–µ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ–≥–æ –∞—Ä—Ö—ñ–≤—É –∑ –±–∞—Ç—á–µ–≤–æ—é –æ–±—Ä–æ–±–∫–æ—é
        
        Args:
            archive_path: –®–ª—è—Ö –¥–æ –∞—Ä—Ö—ñ–≤—É
            progress_callback: –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É
            batch_size: –†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è –æ–±—Ä–æ–±–∫–∏
            skip_existing: –ü—Ä–æ–ø—É—Å–∫–∞—Ç–∏ –≤–∂–µ —ñ—Å–Ω—É—é—á—ñ —Ñ–∞–π–ª–∏
            
        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è
        """
        try:
            if not os.path.exists(archive_path):
                return {'success': False, 'error': f'–ê—Ä—Ö—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {archive_path}'}
            
            print(f"üîç –ü–æ—á–∞—Ç–æ–∫ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è –∞—Ä—Ö—ñ–≤—É: {archive_path}")
            
            # –ó–±—ñ—Ä —Å–ø–∏—Å–∫—É —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏
            files_to_scan = self._collect_files_for_scanning(archive_path, skip_existing)
            
            if not files_to_scan:
                return {
                    'success': True,
                    'message': '–ù–µ–º–∞—î —Ñ–∞–π–ª—ñ–≤ –¥–ª—è —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è',
                    'processed': 0,
                    'errors': 0
                }
            
            print(f"üìÑ –ó–Ω–∞–π–¥–µ–Ω–æ {len(files_to_scan)} —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏")
            
            # –ë–∞—Ç—á–µ–≤–∞ –æ–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—ñ–≤
            results = self._process_files_in_batches(
                files_to_scan, batch_size, progress_callback
            )
            
            # –û—á–∏—â–µ–Ω–Ω—è –∫–µ—à—É –ø—ñ—Å–ª—è —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è
            self._invalidate_search_cache()
            
            return results
            
        except Exception as e:
            error_msg = f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è: {str(e)}"
            print(error_msg)
            return {'success': False, 'error': error_msg}
    
    def _collect_files_for_scanning(self, archive_path, skip_existing=True):
        """–ó–±—ñ—Ä —Å–ø–∏—Å–∫—É —Ñ–∞–π–ª—ñ–≤ –¥–ª—è —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è –∑ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—î—é"""
        files_to_scan = []
        
        try:
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É —ñ—Å–Ω—É—é—á–∏—Ö —Ñ–∞–π–ª—ñ–≤ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø—Ä–æ–ø—É—Å–∫–∞—Ç–∏
            existing_hashes = set()
            existing_paths = set()
            
            if skip_existing:
                existing_hashes, existing_paths = self._get_existing_files_data()
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–∏–π –æ–±—Ö—ñ–¥ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ–π
            for root, dirs, files in os.walk(archive_path):
                # –ü—Ä–æ–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º–Ω–∏—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ–π
                dirs[:] = [d for d in dirs if not is_system_directory(d)]
                
                for file in files:
                    if file.lower().endswith(SUPPORTED_EXTENSIONS):
                        file_path = os.path.join(root, file)
                        
                        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É —Ñ–∞–π–ª—É
                        if get_file_size_mb(file_path) > MAX_FILE_SIZE_MB:
                            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –≤–µ–ª–∏–∫–æ–≥–æ —Ñ–∞–π–ª—É: {file_path}")
                            continue
                        
                        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É
                        if skip_existing:
                            if file_path in existing_paths:
                                continue
                            
                            file_size = int(get_file_size_mb(file_path) * 1024 * 1024)
                            file_hash = self._get_file_hash(file_path, file_size)
                            if file_hash and file_hash in existing_hashes:
                                continue
                        
                        files_to_scan.append(file_path)
            
            return files_to_scan
            
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–æ—Ä—É —Ñ–∞–π–ª—ñ–≤: {e}")
            return []
    
    def _get_existing_files_data(self):
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –ø—Ä–æ —ñ—Å–Ω—É—é—á—ñ —Ñ–∞–π–ª–∏ –∑ –±–∞–∑–∏"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("SELECT file_hash, file_path FROM expertise_cases WHERE file_hash IS NOT NULL")
            data = cursor.fetchall()
            conn.close()
            
            hashes = {row[0] for row in data if row[0]}
            paths = {row[1] for row in data if row[1]}
            
            return hashes, paths
            
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ñ–∞–π–ª—ñ–≤: {e}")
            return set(), set()
    
    def _process_files_in_batches(self, files_list, batch_size, progress_callback=None):
        """–ë–∞—Ç—á–µ–≤–∞ –æ–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—ñ–≤ –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—î—é"""
        total_files = len(files_list)
        processed = 0
        errors = 0
        error_details = []
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –±–∞—Ç—á–µ–≤–æ—ó –≤—Å—Ç–∞–≤–∫–∏
        batch_data = []
        
        try:
            for i, file_path in enumerate(files_list):
                try:
                    # –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    parsed_data = self.parse_expertise_document(file_path)
                    
                    # –î–æ–¥–∞–≤–∞–Ω–Ω—è —Ö–µ—à—É
                    file_size = int(get_file_size_mb(file_path) * 1024 * 1024)
                    parsed_data['file_hash'] = self._get_file_hash(file_path, file_size)
                    
                    batch_data.append(parsed_data)
                    
                    # –û–±—Ä–æ–±–∫–∞ –±–∞—Ç—á—É –ø—Ä–∏ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—ñ —Ä–æ–∑–º—ñ—Ä—É
                    if len(batch_data) >= batch_size:
                        batch_processed, batch_errors = self._save_batch_to_database(batch_data)
                        processed += batch_processed
                        errors += batch_errors
                        batch_data = []
                    
                except Exception as e:
                    errors += 1
                    error_msg = f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ {file_path}: {str(e)}"
                    error_details.append(error_msg)
                    print(f"‚ùå {error_msg}")
                
                # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É
                if progress_callback and (i + 1) % 10 == 0:
                    progress = (i + 1) / total_files
                    progress_callback(progress, f"–û–±—Ä–æ–±–ª–µ–Ω–æ {i + 1}/{total_files} —Ñ–∞–π–ª—ñ–≤")
            
            # –û–±—Ä–æ–±–∫–∞ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –±–∞—Ç—á—É
            if batch_data:
                batch_processed, batch_errors = self._save_batch_to_database(batch_data)
                processed += batch_processed
                errors += batch_errors
            
            return {
                'success': True,
                'total_files': total_files,
                'processed': processed,
                'errors': errors,
                'error_details': error_details[:10]  # –ü–µ—Ä—à—ñ 10 –ø–æ–º–∏–ª–æ–∫
            }
            
        except Exception as e:
            return {'success': False, 'error': f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –±–∞—Ç—á–µ–≤–æ—ó –æ–±—Ä–æ–±–∫–∏: {str(e)}"}
    
    def _save_batch_to_database(self, batch_data):
        """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –±–∞—Ç—á—É –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö"""
        processed = 0
        errors = 0
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ SQL –¥–ª—è –±–∞—Ç—á–µ–≤–æ—ó –≤—Å—Ç–∞–≤–∫–∏
            insert_sql = '''
                INSERT OR IGNORE INTO expertise_cases (
                    erddr_number, expertise_number, expertise_date, expertise_year,
                    expertise_type, expert_name, sector, source_file, file_path,
                    file_size, file_hash, created_at, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
            '''
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            insert_values = []
            for data in batch_data:
                values = (
                    data.get('erddr_number'),
                    data.get('expertise_number'),
                    data.get('expertise_date'),
                    data.get('expertise_year'),
                    data.get('expertise_type'),
                    data.get('expert_name'),
                    data.get('sector'),
                    data.get('source_file'),
                    data.get('file_path'),
                    data.get('file_size', 0),
                    data.get('file_hash'),
                )
                insert_values.append(values)
            
            # –ë–∞—Ç—á–µ–≤–∞ –≤—Å—Ç–∞–≤–∫–∞
            cursor.executemany(insert_sql, insert_values)
            processed = cursor.rowcount
            
            conn.commit()
            conn.close()
            
            print(f"‚úÖ –ë–∞—Ç—á –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {processed} –∑–∞–ø–∏—Å—ñ–≤")
            
        except Exception as e:
            errors = len(batch_data)
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –±–∞—Ç—á—É: {e}")
        
        return processed, errors
